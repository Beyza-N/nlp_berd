!pip install torch
!pip install transformers
!pip install scikit-learn
!pip install matplotlib
import pandas as pd

#from google.colab import drive
#drive.mount('/content/drive')

import torch
from transformers import BertTokenizer, BertForSequenceClassification
from torch.utils.data import DataLoader, TensorDataset
from sklearn.metrics import f1_score

import matplotlib.pyplot as plt

import pandas as pd

def load_excel_data(file_path):
    df = pd.read_excel(file_path, usecols="A", engine='openpyxl')  # Sadece A sütununu oku
    sentences = []
    labels = []
    for _, row in df.iterrows():
        line = row["A"]
        label = int(line[0])  # Etiket, satırın ilk karakteri
        sentence = line[2:].strip()  # Cümle, etiketten sonraki kısım
        sentences.append(sentence)
        labels.append(label)
    return sentences, labels

    train_file_path="/content/train.xlsx"
    test_file_path="/content/test.xlsx"

    emotion_to_label_id= {'negative': 0, 'positive': 1}

    train_sentences, train_labels= load_data(train_file_path)
    test_sentences, test_labels= load_data(test_file_path)

tokenizer= BertTokenizer.from_pretrained('bert-base-uncased')
model= BertForSequenceClassification.from_pretrained('bert-base-uncased',num_labels=3)
def preprocess_data(sentences,labels):
  input= tokenizer(sentences,padding=True,truncation=True,return_tensors='pt')
  labels= torch.tensor(labels)
  return input,labels

train_sentences, train_labels = load_excel_data("/content/train.xlsx")
test_sentences, test_labels = load_excel_data("/content/test.xlsx")


train_input,train_labels= preprocess_data(train_sentences,train_labels)
test_input,test_labels= preprocess_data(test_sentences,test_labels)

train_dataset= TensorDataset(train_inputs['input_ids'], train_inputs['attention_mask'], train_labels)
test_dataset= TensorDataset(test_inputs['input_ids'], test_inputs['attention_mask'], test_labels)

train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)

class BERTContextAwareEmotionRecognition(torch.nn.Module):
    def __init__(self, num_labels):
        super(BERTContextAwareEmotionRecognition, self).__init__()
        self.bert = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=num_labels)

    def forward(self, input_ids, attention_mask):
        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)
        return outputs.logits


def train_model(model, train_loader, num_epochs=5):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)

    train_loss_history = []
    train_acc_history = []
    val_loss_history = []
    val_acc_history = []

    for epoch in range(num_epochs):
        model.train()
        total_loss = 0.0
        correct_predictions = 0

        for input_ids, attention_mask, labels in train_loader:
            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)
            optimizer.zero_grad()
            logits = model(input_ids, attention_mask)
            loss = torch.nn.functional.cross_entropy(logits, labels)
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            predicted_labels = logits.argmax(dim=1)
            correct_predictions += (predicted_labels == labels).sum().item()

        avg_loss = total_loss / len(train_loader)
        train_accuracy = correct_predictions / len(train_dataset)
        train_loss_history.append(avg_loss)
        train_acc_history.append(train_accuracy)

        print(f"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_loss:.4f}, Train Accuracy: {train_accuracy:.4f}")

    return model, train_loss_history, train_acc_history

num_labels = len(emotion_to_label_id)
model = BERTContextAwareEmotionRecognition(num_labels)
trained_model, train_loss_history, train_acc_history = train_model(model, train_loader)



def evaluate_model(model, data_loader):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()
    predictions = []
    true_labels = []

    with torch.no_grad():
        for input_ids, attention_mask, labels in data_loader:
            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)
            logits = model(input_ids, attention_mask)
            predicted_labels = logits.argmax(dim=1)
            predictions.extend(predicted_labels.cpu().numpy())
            true_labels.extend(labels.cpu().numpy())

    f1 = f1_score(true_labels, predictions, average='weighted')
    return f1

test_f1_score = evaluate_model(trained_model, test_loader)
print(f"Test F1 Score: {test_f1_score:.4f}")

def plot_metrics(train_loss_history, train_acc_history, test_acc):
    epochs = range(1, len(train_loss_history) + 1)

    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(epochs, train_loss_history, 'b', label='Training Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training Loss')
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(epochs, train_acc_history, 'r', label='Training Accuracy')
    plt.axhline(y=test_acc, color='g', linestyle='--', label='Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.title('Training and Validation Accuracy')
    plt.legend()

    plt.tight_layout()
    plt.show()

test_accuracy = evaluate_model(trained_model, test_loader)
plot_metrics(train_loss_history, train_acc_history, test_accuracy)


